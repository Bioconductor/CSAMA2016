---
title: "CSAMA 2016: Clustering, classification, and regression with genomic examples"
author: Vince Carey
date: July 13, 2016
output: 
  ioslides_presentation:
    fig_height: 4.8
---
<!--
  ioslides_presentation:
    incremental: false
    fig_height: 3.8
runtime: shiny
-->

## Road map
- use cases
- user interface concepts
- cluster analysis components 
    - primitive sensitivity analysis
- classifier components
    - role of metapackages like caret/mlr/MLInterfaces

## Use case 1: transcript profiles to distinguish tissue source

- illumina bodymap in GEO
- another application: adequacy of mouse models of human biology

## Species and organ of origin: microarrays and orthologues (McCall et al., _NAR_ 2012)

```{r basicbarco,fig=TRUE,echo=FALSE,fig.height=4.4}
suppressPackageStartupMessages({
library(grid)
library(png)
library(MLInterfaces)
library(hgu133a.db)
library(hgu95av2.db)
library(survival)
library(BiocStyle)
library(genefu)
library(drosmap)
library(tissuesGeneExpression)
library(cluster)
library(grid)
library(png)
library(limma)
library(rafalib)
library(fpc)
library(impute)
library(Biobase)
library(tissuesGeneExpression)
library(ggvis)
library(shiny)
})
im = readPNG("figures_vjc/barcoTree.png")
grid.raster(im)
```

## Species, organ of origin, and batch: RNA-seq and orthologues (Lin et al., _PNAS_ 2014)

```{r basicsnypc,fig=TRUE,echo=FALSE,fig.height=3.6}
library(png)
im = readPNG("figures_vjc/snyderPC12.png")
grid.raster(im)
```

- Between-species disparity stronger than within-organ similarity

## Question

* Distinguishing organ of origin through gene expression patterns
    - McCall _et al._, _NAR_ 2011
    - adjusted arrays yield 85 22215-vectors
    - barcode transformation: transcriptomes cluster by organ
* Comparison of human and mouse transcriptomes
    - Lin _et al._, _PNAS_ 2014
    - mRNA abundance for orthologous genes by RNA-seq, 30 15106-vectors
    - transcriptomes cluster by species

Which one is right?



## Use case 2: Oncotype DX gene signature for breast cancer survival

- 21 genes useful for prediction of breast cancer recurrence
- Paik, Shak, Tang et al. NEJM 2004
- `r Biocpkg("genefu")` package includes notation for the signature (`sig.oncotypedx`)
- We'll consider the capacity of the gene set for predicting overall survival in a classic breast cancer dataset (van de Vijver 2002) as packaged in `genefu`


## Setup for NKI breast cancer expression/clinical data

```{r lk1}
library(genefu); library(survival)
data(nkis)
map = as.character(annot.nkis$NCBI.gene.symbol)
names(map) = as.character(annot.nkis$probe)
ndata.nkis = data.nkis
colnames(ndata.nkis) = map[colnames(data.nkis)]
cbind(ndata.nkis[1:4,1:4], demo.nkis[1:4,5:8])
```

## Label expression columns with appropriate symbols; test

```{r docol}
nkSurv = Surv(demo.nkis$t.os, demo.nkis$e.os)
odata = ndata.nkis[, intersect(as.character(sig.oncotypedx$symbol), 
    colnames(ndata.nkis))]
fullnk = cbind(demo.nkis, odata)
coxph(nkSurv~er+age, data=fullnk)
```

## Create a survival tree using all available clinical and expression data {.smaller}

```{r tree}
rfullnk = fullnk[,-c(1,2,3,9,10,11,12,13,14,17,18,19)]
library(rpart); r1 = rpart(nkSurv~.,data=rfullnk)
r1
```

CRAN package `r CRANpkg("partykit")` enhances tree support in `r CRANpkg("rpart")`
and provides many additional models
```{r prunit}
library(partykit)
p1p = as.party(prune(r1, cp=.05))
```

## Visualize the pruned tree along with K-M curves for leaves

```{r lkpar, fig=TRUE, echo=FALSE}
plot(p1p)
```

## Question

What are the key vulnerabilities of an analysis of this type?

## Use case 3: Cell fate signatures from the fruitfly blastocyst


```{r lksiq,fig=TRUE,echo=FALSE}
im = readPNG("siqiTitle.png")
grid.raster(im)
```

## Data setup

```{r getdr}
library(drosmap) # biocLite("vjcitn/drosmap")
data(expressionPatterns)
data(template); template=template[,-1]
data(uniqueGenes)
uex = expressionPatterns[,uniqueGenes]
uex[1:5,1:5]
```

## Spatial gene-specific patterns

```{r lkspa,fig=TRUE}
imageBatchDisplay(uex[,1:16], nrow=4, ncol=4, template=template)
```

## Can we transform spatial patterns for 701 genes to cohere with this fate map?

```{r lksiq2,fig=TRUE,echo=FALSE}
im = readPNG("springmap1.png")
grid.raster(im)
```

## Idea: NMF (Brunet, Tamayo, Golub, Mesirov PNAS 2004) for clustering

```{r nmf,fig=TRUE,echo=FALSE}
im = readPNG("brunetNM.png")
grid.raster(im)
```

## From the `r CRANpkg("NMF")` vignette by Renaud Gaujoux

```{r nmfdef,fig=TRUE,echo=FALSE}
im = readPNG("NMFdef.png")
grid.raster(im)
```

## Factor the matrix of expression measures {.smaller}

* Rows are positions in the reregistered ellipse
* Columns are genes
```{r donm,eval=FALSE}
mm = nmf(uex, rank=21) # takes a minute on macbook
```
```
<Object of class: NMFfit>
 # Model:
  <Object of class:NMFstd>
  features: 405 
  basis/rank: 21 
  samples: 701 
 # Details:
  algorithm:  brunet 
  seed:  random 
  RNG: 403L, 1L, ..., 1716923164L [baff3023b8693dbef07d065d4e2b4db6]
  distance metric:  'KL' 
  residuals:  2766.095 
  Iterations: 2000 
  Timing:
     user  system elapsed 
   72.953   3.245  77.848 
```

## Project the basis vectors to the blastocyst template

```{r nodo,eval=FALSE}
imageBatchDisplay(basis(mm), nrow=5, ncol=5, template=template)
```
```{r mynnn,fig=TRUE,echo=FALSE}
im = readPNG("mynmf.png")
grid.raster(im)
```

## An assignment of "principal patterns"

```{r lippp,fig=TRUE,echo=FALSE}
im = readPNG("fullFateMap.png")
grid.raster(im)
```

## Comments

* *Curse of dimensionality:* as the number of features increases, utility
of distance metrics for object grouping diminishes (space is mostly
empty, distances generally small) 
* *Bet on sparsity principle:* favor procedures that are able to prune
features/dimensions, because in non-sparse case, nothing works
* All the results displayed are tunable, could be interactive
* Sensitivity analysis: Enhance the capacity of reports to demonstrate
their own robustness

## Remainder of talk

* Bioconductor strategies: user interface and object designs
* Cluster analysis formalities; hclustWidget
* Classifier formalities; mlearnWidget

## On the user interface

* The method is primary (constituents of CRAN task view "MachineLearning")
* What does the learner consume?
    - data in a specific format, tuning parameters
* What does the learner emit?
    - an object with scores, assignments, metadata about the run
* Aims
    - reduce complexity of user tasks
    - capitalize on formal structuring of containers for inputs and outputs
    - foster sensitivity analysis
* We'll now use a modified MLInterfaces::hclustWidget that capitalizes on these notions


```{r sta, echo=FALSE}
data(tissuesGeneExpression)
library(Biobase)
tiss = ExpressionSet(e)
rownames(tab) = colnames(e)
pData(tiss) = tab
tiss = tiss[, tiss$SubType == "normal"]
annotation(tiss) = "hgu133a.db"
etiss = exprs(tiss)
colnames(etiss) = tiss$Tissue
#datatable(pData(tiss))
```

## Exploring clusters with tissue-of-origin data


```{r mkhc,echo=FALSE}
nicehclustWidget = function(mat) {
 shinyApp(ui = fluidPage(
  fluidRow(
   column(3,  numericInput("ngenes", label = "N genes:", 100, min = 2, max = 100000)),
   column(3,  selectInput("distmeth", label = "Distance method:",
               choices = c("euclidean", "maximum", "manhattan",
               "binary"), selected = "euclidean")),

   column(3,  selectInput("fusemeth", label = "Agglomeration method:",
               choices = c("complete", "average", "ward.D2", "single",
                   "median", "centroid"), selected="complete")),
   column(3,  numericInput("numclus", label = "K:", 6, min = 2, max = 9))
          ),
  fluidRow(column(8, plotOutput("tree")), column(4, ggvisOutput("pcp")))
 ), server= function(input, output, session) {
    output$tree <- renderPlot({
dm = dist(mat[,1:input$ngenes], method=input$distmeth)
sink(tempfile())
cb <- clusterboot(dm, clustermethod=hclustCBI, method=input$fusemeth, k=input$numclus, showplots=FALSE, scaling=FALSE)
sink(NULL)
      dend = hclust( dm, method=input$fusemeth )
      par(mar=c(3,3,3,1))
      mnbc = round(rmnbc <- mean(cb$bootmean),2)
      plot(dend, main=paste0("Boot. Jacc. at k=", input$numclus, ": ",
        paste("mean: ", mnbc, collapse=""), "; resids:",
        paste(round(rmnbc - cb$bootmean,2), collapse=", ")), xlab=" ")
    })
    P1 <- reactive({
           all_values <- function(x) {
             if(is.null(x)) return(NULL)
             row <- pcdf[pcdf$rowid == x$rowid, ]
             paste0(names(row), ": ", format(row), collapse = "<br />")
           }

      pc = prcomp(mat[,1:input$ngenes])$x
      dm = dist(mat[,1:input$ngenes], method=input$distmeth)


      dend = hclust( dm, method=input$fusemeth )
      ct = cutree(dend, k=input$numclus)
      pcdf = data.frame(PC1=pc[,1], PC2=pc[,2], tiss=pData(tiss)$Tissue,
         rowid=1:nrow(pc), assigned=factor(ct))
      pcdf %>% ggvis(~PC1, ~PC2, key := ~rowid, fill = ~assigned) %>% layer_points() %>%
               add_tooltip(all_values, "hover") 
#      pairs(pc[,1:3], col=ct, pch=19, cex=1.5)
      }) 
      P1 %>% bind_shiny("pcp")
} )
}
```

```{r lkwid, echo=FALSE}
nicehclustWidget(t(etiss))
```

## Some definitions: general distance

```{r distdef,fig=TRUE,echo=FALSE,fig.height=5.2}
library(png)
im = readPNG("figures_vjc/metricDef.png")
grid.raster(im)
```


## Examples: 

### Euclidean distance

- High-school analytic geometry: distance between two points in $R^3$
- $p_1 = (x_1, y_1, z_1)$, $p_2 = (x_2, y_2, z_2)$
- $\Delta x = x_1 - x_2$, etc.
- $d(p_1, p_2) = \sqrt{(\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2}$

### Manhattan distance

- $d(p_1, p_2) = |\Delta x| + |\Delta y| + |\Delta z|$

### New concept of distance for categorical vectors:

Sam Buttrey and Lyn Whitaker's `r CRANpkg("treeClust")` ([R Journal article](https://journal.r-project.org/archive/2015-2/buttrey-whitaker.pdf))

## What is the ward.D2 agglomeration method?
- Enables very rapid update upon change of distance or # genes

```{r wardalg,fig=TRUE,echo=FALSE,fig.height=5.2}
library(png)
im = readPNG("figures_vjc/wards.png")
grid.raster(im)
```

## What is the Jaccard similarity coefficient?

```{r jacc,fig=TRUE,echo=FALSE,fig.height=5.2}
library(png)
im = readPNG("figures_vjc/jaccdef.png")
grid.raster(im)
```

## Summary

* Hierarchical clustering is tunable; distance, fusion
method, feature selection all have impact
* There are other principles/algorithms: divisive, semi-supervised, model-based
* Other figures of merit: consensus, gap statistic
* See the `r CRANpkg("mlr")` for structured interface


## On classification methods with genomic data

- Vast topic
- Key resources in R:
    - Machine Learning [task view](http://cran.r-project.org/web/views/MachineLearning.html) at CRAN
    - 'metapackage' [mlr](http://cran.r-project.org/web/packages/mlr/index.html)
- In Bioconductor, consider
    - The 'StatisticalMethod' task view (next slide)
    - MLInterfaces (a kind of metapackage)

## BiocViews: StatisticalMethod

```{r statmv,fig=TRUE,echo=FALSE,fig.height=4.6}
library(png)
im = readPNG("figures_vjc/statmeth.png")
grid.raster(im)
```

## Conceptual basis for methods covered in the talk

- "Two cultures" of statistical analysis (Leo Breiman)
    - model-based 
    - algorithmic

- Ideally you will understand and use both
    - $X \sim N_p(\mu, \Sigma)$, seek and use structure in $\mu$, $\Sigma$ as estimated from data; pursue weakening of model assumptions
    - $y \approx f(x)$ with response $y$ and features $x$, apply agnostic algorithms to the data to choose $f$ and assess the quality of the prediction/classification

## A method on the boundary: linear discriminant analysis

- The idea is that we can use a linear combination of features to define a score for each object 
- The value of the score determines the class assignment
- This assumes that the features are quantitative and are measured consistently for all objects 
- for $p$-dimensional feature vector $x$ with prior probability $\pi_k$, mean $\mu_k$ for class $k$, and
common covariance matrix for all classes
$$
\delta_k(x) = x^t\Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^t \Sigma^{-1} \mu_k + \log \pi_k
$$
is the discriminant function; $x$ is assigned to the class for which $\delta_k(x)$ is largest

## Notes on LDA

- It is "on the boundary" because it can be justified using parametric modeling
assumptions, assigning to maximize likelihood ratio
- Algorithmic arguments justify the criterion as it maximizes ratio of between- to within-class variances among all linear combinations of features (Fisher)
- Further algorithmic arguments lead to variations based on regularization concepts

## Other approaches, issues

- Direct "learning" of statistical parameters in regression or
neural network models
- Recursive partitioning of classes, repeating searches through all
features for optimal discrimination
- Ensemble methods in which votes are assembled among different learners
or over perturbations of the data
- Unifying loss-function framework: see _Elements of statistical learning_ by
Hastie, Tibshirani and Friedman
- Figures of merit: misclassification rate (cross-validated), AUROC 

## A demonstration with tissue-of-origin expression data

```{r lkmlw,echo=FALSE}
mlearnWidget(tiss, infmla=Tissue~.)
```

## check out mlr and consider how MLInterfaces could employ it

```{r lkbi,echo=FALSE}
im = readPNG("mlrTop.png")
grid.raster(im)
```


## Remarks

* all examples here employ mature, reduced data
* statistical learning also important at early stages, but data volume
leads to challenges
* interactive modeling/learning as the product
   - in opposition to a potentially overoptimistic selection
* new work on post-selection inference in `r CRANpkg("selectiveInference")`


<!--
3) components of a cluster analysis: vectors (quantitative or
   categorical), feature selection/weighting,
   distance metric, amalgamation or partitioning
   algorithm, figure of merit, assignments/scores

4) hclustWidget -- a tool for limited sensitivity analysis

5) components of a classifier: training set (vectors with class
   labels), feature selection procedure, selection of tuning parameters,
   algorithm execution, training assignments, deployment on test
   data, evaluation of figure of merit


Some useful software
  - Bioconductor MLInterfaces lets you explore various approaches with
genomic data fairly conveniently
  - more recently, Bischl's mlr package comprehensively "metapackages"
CRAN learners and more
  - we've already had a reference to caret, another metapackaging package

Some formalism for hierarchical clustering
   - distance
   - Ward's D2 and iterative agglomeration
   - cutting the tree for labeling purposes

Projecting the labeled data
   - PCA
   - biplots

Some formalism for tree-based learning
   - CART: recursive partitioning to maximize node purity
   - cost-complexity pruning: minimize error rate plus penalty for tree size
   - classifier: drop a vector down the fitted tree
   - random forests: ensembles of trees created through resampling, voting
       to create assignments
   - classifier: drop a vector down all the trees and tally votes

-->
