% To compile this:
% rm(list=ls()); library('knitr'); pat_md(); knit('multtest.Rnw')

%------------------------------------------------------------
\chapter{Multiple Testing}\label{Chap:14}
%------------------------------------------------------------
```{r initialize, cache = FALSE, echo = FALSE, message = FALSE, error = FALSE, warning = FALSE}
source("../chapter-setup.R"); chaptersetup("multtest.Rnw", "14")
```

\index{multiple testing}
\begin{marginfigure}
\includegraphics[width=\linewidth]{xkcdmulttest-newspapertitle}
\caption{From \url{http://xkcd.com/882}}\label{fig:mt:jellybeans}
\end{marginfigure}
Hypothesis testing is one of the workhorses of science. It is how we draw conclusions or
make decisions based on finite samples of data. For instance, new drugs are usually
approved on the basis of clinical trials that aim to decide whether the drug has better
efficacy (and an acceptable trade-off of side effects) compared to the other available
options. Such trials are expensive and can take a long time. Therefore, the number of
patients we can enroll is limited, and we need to base our inference on a limited sample
of observed patient responses. The sample needs to be big enough allow us to make a
reliable conclusion, but small enough not to waste precious resources or time. The
machinery of hypothesis testing was developed largely with this application in
mind, although today it is used much more
widely.

%------------------------------------------------------------
\section{Goals for this Chapter}
%------------------------------------------------------------
\begin{itemize}
\item Familiarize ourselves with the machinery of hypothesis testing,
its vocabulary, its purpose, and its strengths and limitations.
\item Understand what multiple testing means.
\item See that multiple testing is not a problem --- but rather, an opportunity, as it fixes many of the
limitations of single testing.
\item Understand the false discovery rate.
\item Learn how to make diagnostic plots.
\item Use hypothesis weighting to increase the power of our analyses.
\end{itemize}

\subsection{Drinking from the firehose}
If statistical testing ---reasoning with uncertainty--- seems a hard task if you do it for
one single decision (or test), then brace yourself: in genomics, or more generally with
``big data'', we need to accomplish it not once, but thousands or millions of
times. You've already seen in this Chapter 7, where we analysed RNA-Seq data for
differential expression. We applied a hypothesis test to each of the genes, that is, we
did several thousand tests. Similarly, in whole genome sequencing, we scan every position in the
genome for a difference between the sample at hand and a reference (or, another sample):
that's on the order of 3 billion tests if we are looking at human data! In RNAi or
chemical compound screening, we test each of the reagents for an effect in the assay,
compared to control: that's again tens of thousands, if not millions of tests.

\begin{marginfigure}
\includegraphics[width=\linewidth]{hydrantwater.jpg}
\caption{Modern biology often involes navigating a deluge of data.
\href{http://ak1.picdn.net/shutterstock/videos/945751/preview/stock-footage-open-butt-hydrant-flowing-water-into-a-industrial-pond.jpg}{Source}}
\end{marginfigure}

Yet, in many ways, the task becomes simpler, not harder. Since we have so much
data, and so many tests, we can ask questions like: are the assumptions
of the tests actually met by the data? What are the prior probabilities that we should
assign to the possible outcomes of the tests? Answers to these questions can be incredibly
helpful, and we can address them \emph{because} of the multiplicity. So we should think about
it not as a ``multiple testing problem'', but as an opportunity!

There is a powerful premise in
data-driven sciences: we usually expect that most tests will not be rejected. Out of the
thousands or millions of tests (genes, positions in the genome, RNAi reagents), we expect
that only a small fraction will be interesting, or ``significant''. In fact, if
that is not the case, if the hits are not rare, then arguably our analysis method --serially univariate
screening of each variable for association with the outcome--
is not suitable for the dataset. Either we need better
data (a more specific assay), or a different analysis method, e.\,g., a multivariate model.

Since most nulls are true, we can use the behaviour of the many test statistics and
p-values to empirically understand their null distributions, their correlations, and so
on. Rather than having to rely on \emph{assumptions} we can check them empirically!

\subsection{Testing vs classification}

There are many methodological similarities between hypothesis testing and classification, but
the differences are good to keep in mind.  In both cases, we aim to use data to
choose between several possible decisions. For instance, we might use the measured
expression level of a marker gene to decide whether the cells we're studying are
from cell
type A or B. If we have no prior assumption, and if we're equally
worried about mistaking
an A for a B, or vice versa, then we're best served by the machinery of
classification as covered briefly in Chapter~\ref{Chap:16}
and in detail in~\cite{HastieTibshiraniFriedman}.  On the other
hand, if --before seeing the data-- we have a preference for A, and need evidence to be
convinced otherwise, then the machinery of hypothesis testing is right for us. For
instance, if a disease is currently treated with some established medication, and someone
comes up with a proposal to treat it with a different treatment instead, the burden of
proof should be with them, and the data should prove the benefit of the new treatment
with high certainty. We can also think of this as an application of
\eindex{Occam's razor}\footnote{See also
  \url{https://en.wikipedia.org/wiki/Occam\%27s_razor}} -- don't come up with a more
complicated solution if a simpler one does the job.

%------------------------------------------------------------
\section{An Example: Coin Tossing}
%------------------------------------------------------------
To understand multiple tests, let's first review the mechanics of
single hypothesis testing.
For example, suppose we are flipping a coin to see if it is a fair coin\footnote{The same
  kind of reasoning, just with more details, applies to any kind of gambling. Here we
  stick to coin tossing since everything can be worked out easily, and it shows all the
  important concepts.}.  We flip the coin 100 times and each time record whether it came
up heads or tails. So, we have a record that could look something like this:

\texttt{H H T T H T H T T ...}

Which we can simulate in \texttt{R}. We set \Robject{probHead} different from 1/2, so we
are sampling from a biased coin: \label{mt:probHead}
%
```{r whatprob1}
set.seed(0xdada)
numFlips <- 100
probHead <- 0.6
coinFlips <- sample(c("H", "T"), size = numFlips,
  replace = TRUE, prob = c(probHead, 1 - probHead))
head(coinFlips)
```
%
Now, if the coin were fair, we expect half of the time to get heads. Let's see.
%
```{r tableCoinFlips}
table(coinFlips)
```
%
So that is different from 50/50. Suppose we didn't know whether the coin is fair or not --
but our prior assumption is that coins are, by and large, fair:
would these observed data be strong enough to make us conclude that this coin isn't fair?
We know that random sampling differences are to be expected. To decide, let's
look at the sampling distribution of our test statistic --the total number of heads seen
in `r numFlips` coin tosses-- for a fair coin\footnote{We haven't really defined what we
  mean be fair -- a reasonable definition would be that head and tail are equally likely,
  and that the outcome each coin toss is completely independent of the previous ones. For
  more complex applications, nailing down the exact null hypothesis can take a bit more
  thought.}. This is really easy to work out with elementary combinatorics:
\begin{equation}\label{eq:mt:dbinom}
P(K=k\,|\,n, p) = \left(\begin{array}{c}n\\k\end{array}\right) p^k\;(1-p)^{n-k}
\end{equation}
Let's parse the notation: $n$ is the number of coin tosses (`r numFlips`) and $p$ is the
probability of head (0.5 if we assume a fair coin).  $k$ is the number of
heads. Statisticians like to make a difference between all the possible values of a
statistic and the one that was observed, and we use the lower case $k$ for the possible
values (so $k$ can be anything between 0 and `r numFlips`), and the upper case $K$ for the observed
value. We pronounce the left hand side of the above equation as ``the probability that the
observed number takes the value $k$, given that $n$ is what it is and $p$ is what it is''.

Let's plot Equation~\eqref{eq:mt:dbinom}; for good measure, we also mark the observed value
\Robject{numHeads} with a vertical blue line.
%
```{r binomDens}
k <- 0:numFlips
numHeads <- sum(coinFlips == "H")
binomDensity <- data.frame(k = k,
     p = dbinom(k, size = numFlips, prob = 0.5))
```
```{r dbinom, fig.width=3.5, fig.height=3}
library("ggplot2")
ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p), stat = "identity") +
  geom_vline(xintercept = numHeads, col="blue")
```
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_dbinom-1}
\caption{The binomial distribution for the parameters $n=`r numFlips`$ and $p=0.5$, 
according to Equation~\eqref{eq:mt:dbinom}.}
\label{fig:mt:dbinom}
\end{marginfigure}
%
Suppose we didn't know about Equation~\eqref{eq:mt:dbinom}. We could still manoeuvre our way out by
simulating a reasonably good \emph{approximation} of the distribution.
%
```{r rbinom, fig.width=3.5, fig.height=3}
numSimulations <- 10000
outcome <- replicate(numSimulations, {
  coinFlips <- sample(c("H", "T"), size = numFlips,
    replace = TRUE, prob = c(0.5, 0.5))
  sum(coinFlips == "H")
})
ggplot(data.frame(outcome)) + xlim(0, 100) +
  geom_histogram(aes(x = outcome), binwidth = 1, center = 0) +
  geom_vline(xintercept = numHeads, col="blue")
```
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_rbinom-1}
\caption{An approximation of the binomial distribution from `r numSimulations` simulations
(same parameters as Figure~\ref{fig:mt:dbinom}).}
\end{marginfigure}
%

As expected, the most likely number of heads is `r numFlips/2`, that is, half the number
of coin flips.  But we see that other numbers near `r numFlips/2` are also not
unlikely. How do we quantify whether the observed value, `r numHeads`, is among those
values that we are likely to see from a fair coin, or whether its deviation from the
expected value is already big enough for us to conclude with enough confidence that the
coin is biased? We divide the set of all possible $k$'s (0 to `r numFlips`) in two
complementary subsets, the \eindex{acceptance region} and the
\eindex{rejection region}.  A natural choice\footnote{More on this below.}
is to fill up the rejection region with as many $k$ as
possible while keeping the total probability below some threshold $\alpha$ (say, 0.05). So
the rejection set consists of the values of $k$ with the smallest probabilities
\eqref{eq:mt:dbinom}, so that their sum remains $\le\alpha$.

```{r findrej, fig.width=3.5, fig.height=3}
library("dplyr")
alpha <- 0.05
binomDensity <- arrange(binomDensity, p) %>%
        mutate(reject = (cumsum(p) <= alpha))

ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p, col = reject), stat = "identity") +
  scale_colour_manual(
    values = c(`TRUE` = "red", `FALSE` = "darkgrey")) +
  geom_vline(xintercept = numHeads, col="blue") +
  theme(legend.position = "none")
```
%$
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_findrej-1}
\caption{As Figure~\ref{fig:mt:dbinom}, with rejection region (red) that has been chosen
  such that it contains the maximum number of bins whose total area is at most 
  $\alpha=`r alpha`$.}
\label{fig:mt:findrej}
\end{marginfigure}
%
In the code above, we used the functions \Rfunction{arrange} and \Rfunction{mutate} from the
\CRANpkg{dplyr} package to sort the the p-values from lowest to highest, compute the cumulative
sum (\Rfunction{cumsum}), and stop rejecting once it exceeds \Robject{alpha}.

The explicit summation over the probabilities is clumsy, we did it here for pedagogic
value. For one-dimensional distributions, R provides not only functions for
the densities (e.\,g., \Robject{dbinom}) but also for the cumulative distribution
functions (\Rfunction{pbinom}), which are more precise and faster than \Rfunction{cumsum}
over the probabilities. These should be used in practice.

We see in Figure~\ref{fig:mt:findrej} that the observed value, `r numHeads`, lies in the
grey shaded area, so we would \emph{not} reject the null hypothesis of a fair coin from
these data at a significance level of $\alpha=`r alpha`$.
\begin{ques}
Does the fact that we don't reject the null hypothesis mean that the coin is fair?
\end{ques}
\begin{ques}
Would we have a better chance of detecting that the coin is not fair if we did more coin tosses? How many?
\end{ques}
\begin{ques}
  If we repeated the whole procedure and again tossed the coin `r numFlips` times, might
  we \emph{then} reject the null hypothesis?
\end{ques}
\begin{ques}
The rejection region in Figure~\ref{fig:mt:findrej} is asymmetric - its left part ends with
$k=`r (.tmp1 <- max(filter(binomDensity, reject & k < numFlips/2)$k))`$,
while its right part starts with
$k=`r (.tmp2 <- min(filter(binomDensity, reject & k > numFlips/2)$k))`$.
Why is that? Which other ways of defining the rejection region might be useful?
\end{ques}
```{r assertion, echo=FALSE}
stopifnot( .tmp1 + .tmp2 != numFlips )
```

The binomial test is such a frequent activity that it has been wrapped into a single
function, and we can compare its output to our results

```{r binom.test}
binom.test(x = numHeads, n = numFlips, p = 0.5)
```

%--------------------------------------------------
\section{The Five Steps of Hypothesis Testing} \label{sec:mt:fivesteps}
%--------------------------------------------------
Let's summarise the general principles\footnote{These are idealised; 
for a reality check, see below, Section~\ref{sec:mt:pvaluehack}.}
of hypothesis testing:
\begin{enumerate}
\item Choose an experimental design and a data summary function for the effect that you are
interested in, the \eindex{test statistic}.
\item \label{HT:stepcomp} Set up a \eindex{null hypothesis}, which is a simple, computationally
tractable model of reality that lets you compute the \eindex{null distribution}, i.\,e., the 
possible outcomes of the test statistic and their probabilities.
\item Decide on the \eindex{rejection region}, i.\,e., a subset of possible outcomes whose
  total probability is small.
\item Do the experiment, collect data, compute the test statistic.
\item Make a decision: reject the null hypothesis if the test statistic is in the rejection region.
\end{enumerate}

The null hypothesis we used in the coin tossing example \marginpar{Null hypothesis} was
that heads and tails are equally likely, and that the outcome of each coin toss is
independent of the previous ones. This is idealized: a real coin might have some, if ever
so slight irregularities, so that the probability of head might be 0.500001; but here
we don't worry about that, nor about any possible effects of air drag, elasticity of the
material on which the coin falls, and so on. It is also computationally tractable, namely,
with the binomial distribution.

The test statistic in our example was the total number of heads.  \marginpar{Test
  statistic} Suppose we observed 50 tails in a row, and then 50 heads in a row.  Our test
statistic ignores the order of the outcomes, and we would conclude that this is a
perfectly fair coin. However, if we used a different test statistic (say, the number of
times we see two tails in a row), we might notice that there is something funny about this
coin.
\begin{ques}
What is the null distribution of this different test statistic?
\end{ques}
\begin{ques}
Would a test based on that statistic be generally preferable?
\end{ques}

What we have just done is that we looked at two different classes of \eindex{alternative
  hypotheses}. \marginpar{Alternative hypotheses} The first class of alternatives was that
subsequent coin tosses are still independent of each other, but that the probability of
heads differed from 0.5 The second one was that the overall probability of heads may
still be 0.5, but that subsequent coin tosses were correlated.
\begin{ques}
Recall the concept of sufficient statistics from Chap\ref{Chap:10}. % Design
Is the total number of heads a sufficient statistic for the binomial distribution?
Why might be it be a good test statistic for our first class of alternatives, but not for the second?
\end{ques}
\begin{ques}
Does a test statistic always have to be sufficient?
\end{ques}

So let's remember that we typically have multiple possible choices of test statistic (in
principle it could be any numerical summary of the data). Making the right choice is
important for getting a test with good power\footnote{See
  Section~\ref{sec:mt:typesoferror}.}. What the right choice is will depend on what kind
of alternatives we expect. This is not always easy to know in advance.

Once we have chosen the test statistic we need to compute its null distribution. You can
do this either with pencil and paper or by computer simulations. \marginnote{Parametric
  theory versus simulation} A pencil and paper solution that leads to a closed form
mathematical expression (like Equation~\eqref{eq:mt:dbinom}) has the advantage that it holds
for a range of model parameters of the null hypothesis (such as $n$, $p$). And it can be
quickly computed for any specific set of parameters. But it is not always as easy as in
the coin tossing example. Sometimes a pencil and paper solution is impossibly difficult to
compute. At other times, it may require simplifying assumptions. An example is a null
distribution for the $t$-statistic (which we will see later in this chapter). We can
compute one if we assume that the data are independent and Normal distributed, the result
is called the $t$-distribution. Such modelling assumptions may be more or less
realistic. Simulating the null distribution offers a potentially more accurate, more
realistic and perhaps even more intuitive approach.  The drawback of simulating is that it
can take a rather long time, and we have to work extra to get a systematic understanding
of how varying parameters influence the result.  Generally, it is more elegant to use the
parametric theory when it applies\footnote{The assumptions don't need to be \emph{exactly} true -- 
it is sufficient if the theory's predictions are an acceptable approximation of the truth.}.
When you are in doubt, simulate -- or do both.

As for the rejection region: how small is small enough? \marginnote{Rejection region} That
is your choice of the \eindex{significance level} $\alpha$, which is the total probability
of the test statistic falling into this region if the null hypothesis is
true\footnote{Some people at one point in time for a particular set of questions colluded
  on $\alpha=0.05$ as being ``small''. But there is nothing special about this
  number.}. Even when $\alpha$ is given, the choice of the rejection region is not unique.
A further condition that we require from a good rejection region is that the probability
of the test statistic falling into it is as large possible if the null hypothesis is
indeed false. In other words, we want our test to have high \eindex{power}.

In Figure~\ref{fig:mt:findrej}, the rejection region is split between the two tails of the
distribution. This is because we anticipate that unfair coins could have a bias either
towards head or toward tail; we don't know. If we did know, we could instead concentrate
our rejection region all on the appropriate side, e.\,g., the right tail if we think the
bias would be towards head. Such choices are also refered to as
\emph{two-sided}\index{two-sided test} and \emph{one-sided}\index{two-sided test} tests.


%------------------------------------------------------------
\section{Types of Error}\label{sec:mt:typesoferror}
%------------------------------------------------------------
Having set out the mechanics of testing, we can assess how well we are doing.
Table~\ref{tab:mt:typeosferror} compares reality (whether or not the null hypothesis is in fact true)
with the decision whether or not to reject it.

\begin{table}
\begin{longtable}[c]{@{}lll@{}}
\hline\noalign{\medskip}
Test vs reality & \textbf{Null hypothesis is true} &
                  \textbf{\ldots is false} \\\noalign{\medskip}
\hline\noalign{\medskip}
\textbf{Reject null hypothesis} & Type I error (false positive) & True
positive \\\noalign{\medskip}
\textbf{Do not reject} & True negative & Type II error
(false negative)
\\\noalign{\medskip}
\hline
\end{longtable}
\caption{Types of error in a statistical test.}\label{tab:mt:typeosferror}
\end{table}

```{r errortypesdemo, fig.width = 3.5, fig.height = 2.75, echo = FALSE}
library("RColorBrewer")
.localVars <- c("pcut", "i1", "i2", "f1", "f2", "px")
stopifnot(!any(sapply(.localVars, exists, where = .GlobalEnv)))

pcut <- 5
px <- seq(0, 11, length.out = 100)
f1 <- dgamma(px, shape = 2, rate = 0.8)
f2 <- dnorm(px, mean = 7, sd = 1.2)
i1 <- which(px <= pcut)
i2 <- i1[length(i1)]:length(px)

list(
  data_frame(
    x = px[c(i1, rev(i1))],
    y = c(f2[i1], f1[rev(i1)]),
    ` ` = "TN"),
  data_frame(
    x = px[c(i1, rev(i1))],
    y = c(rep(0, length(i1)), f2[rev(i1)]),
    ` ` = "FN"),
  data_frame(
    x = px[c(i2, rev(i2))],
    y = c(f1[i2], f2[rev(i2)]),
    ` ` = "TP"),
  data_frame(
    x = px[c(i2, rev(i2))],
    y = c(rep(0, length(i2)), f1[rev(i2)]),
    ` ` = "FP")) %>% do.call(rbind, .) %>%
ggplot(aes(x = x, y = y, fill = ` `)) + geom_polygon() +
  scale_fill_manual(values = brewer.pal(12, "Paired")[c(2, 1, 6, 5)] %>%
     `names<-`(c("FN", "TN", "FP", "TP"))) +
  geom_vline(xintercept = px[i2[1]], col = "black", size = 1) +
  xlab("test statistic") +
  theme(legend.position="none")
rm(list = .localVars)
```

The two types of error we can make are in the lower left and upper right cells of the
table. It's always possible to reduce one of the two error types on the cost of increasing
the other one.  The real challenge is to find an acceptable trade-off between both of
them. This is exemplified in Figure~\ref{fig:mt:typesoferror}.  We can always decrease the
\eindex{false positive rate} (FPR) by shifting the threshold to the right.  We can become 
more ``conservative''. But this happens at the price of higher 
\eindex{false negative rate} (FNR). Analogously, we can decrease the FNR by shifting the 
threshold to the left. But then again, this happens at the price of higher FPR.
%
A bit on terminology: the FPR is the same as the probability $\alpha$ that we mentioned above.
$1 - \alpha$ is also called the \eindex{specificity} of a test.
The FNR is sometimes also called $\beta$, and
$1 - \beta$ the \eindex{power}, \eindex{sensitivity} or \eindex{true positive rate} of a test.

\begin{ques}
\item At the end of Section~\ref{sec:mt:fivesteps} we learned about one- and two-sided
  tests.  Why does this distinction exist -- why don't we alway just use the two-sided
  test, which is sensitive to a larger class of alternatives?
\end{ques}

\begin{marginfigure}[-12mm]
\includegraphics[width=\linewidth]{chap14-r_errortypesdemo-1}
\caption{The trade-off between type I and II errors. 
The densities represent the distributions of a hypothetical test statistic either under the null or the 
alternative. The  peak on the left (light and dark blue plus dark red) represents the 
test statistic's distribution under the null. It integrates to 1.
Suppose the decision boundary is the black line and
the hypothesis is rejected if the statistic falls to the left. The probability of a false
positive (the FPR) is then simply the dark red area.
Similarly, if the peak on the right (light and dark red plus dark blue area) is the 
test statistic's distribution under the alternative, the probability of
a false negative (the FNR) is the dark blue area.}
\label{fig:mt:typesoferror}
\end{marginfigure}


%--------------------------------------------------
\section{The t-test}\label{sec:mt:ttest}
%--------------------------------------------------
Many experimental measurements are reported as real numbers, and the simplest comparison
we can make is between two groups, say, cells treated with a substance compared to
cells that are not. The basic test for such situations is the $t$-test. The test statistic is
defined as
\begin{equation}\label{eq:mt:tstat}
t = c \; \frac{m_1-m_2}{s},
\end{equation}
where $m_1$ and $m_2$ are the mean of the values in the two groups,
$s$ is the pooled standard deviation and $c$ is a constant that depends on
the sample sizes, i.\,e., the numbers of samples $n_1$ and $n_2$ in the two
groups. To be totally explicit,
\begin{eqnarray}
m_g  &=& \frac{1}{n_g} \sum_{i=1}^{n_g} x_{g, i} \quad\quad\quad g=1,2\nonumber\\
s^2 &=& \frac{1}{n_1+n_2-2}
   \left( \sum_{i=1}^{n_1} \left(x_{1,i} - m_1\right)^2 +
          \sum_{j=1}^{n_2} \left(x_{2,j} - m_2\right)^2 \right)
\nonumber\\
c   &=& \sqrt{\frac{n_1n_2}{n_1+n_2}}.\label{eq:mt:cttest}
\end{eqnarray}
%
```{r checkbyexperimentalmaths, echo=FALSE, results="hide"}
.myttest <- function(x, y) {
  mx  <- mean(x)
  my  <- mean(y)
  s12 <- sqrt((sum((x-mx)^2)+sum((y-my)^2)) / (length(x)+length(y)-2))
  (mx - my) / s12 * sqrt(length(x)*length(y)/(length(x)+length(y)))
}
replicate(100, {
  x <- rnorm(ceiling(30 * runif(1)))
  y <- rnorm(ceiling(30 * runif(1)))
  stopifnot(abs(.myttest(x, y) - t.test(x, y, var.equal=TRUE)$statistic) < 1e-9)
})
```
%$
where $x_{g, i}$ is the $i^{\text{th}}$ data point in the $g^{\text{th}}$ group. Let's try this
out with the \Robject{PlantGrowth} data from R's \CRANpkg{datasets} package.
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_plantgrowth-1}
\caption{The \Robject{PlantGrowth} data.\label{fig:mt:plantgrowth}}
\end{marginfigure}
%
```{r plantgrowth, fig.width = 3, fig.height = 2.75}
data("PlantGrowth")
ggplot(PlantGrowth, aes(y = weight, x = group, col = group)) + 
  geom_jitter(height = 0, width = 0.4) + 
  theme(legend.position = "none")
tt <- with(PlantGrowth,
   t.test(weight[group =="ctrl"], 
          weight[group =="trt2"],
          var.equal = TRUE)) 
tt
```
%
\begin{ques}
What do you get from the comparison with \Robject{trt1}? What for \Robject{trt1} versus \Robject{trt2}?
\end{ques}
\begin{ques}
What is the significance of the \Robject{var.equal = TRUE} in the above call to \Rfunction{t.test}?
\end{ques}
\begin{ques}
Rewrite the above call to  \Rfunction{t.test} using the formula interface, i.\,e., by using the notation
\Robject{weight $\sim$ group}.
\end{ques}
%
To compute the p-value, the \Rfunction{t.test} function uses the asymptotic theory
for the $t$-statistic~\eqref{eq:mt:tstat}; this theory states that under the null hypothesis of
equal means in both groups, this quantity follows a known, mathematical distribution, the
so-called $t$-distribution with $n_1+n_2$ degrees of freedom. The theory uses additional
technical assumptions, namely that the data are independent and come from a Normal
distribution with the same standard deviation. We could be worried about these
assumptions. Clearly they do not hold: weights are always positive, while the Normal
distribution extends over the whole real axis. The question is whether this deviation from
the theoretical assumption makes a real difference. We can use sample permutations to figure this out.
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_ttestperm-1}
\caption{The null distribution of the (absolute) $t$-statistic determined by simulations -- 
namely, by random permutations of the group labels.\label{fig:mt:ttestperm}}
\end{marginfigure}
%
```{r ttestperm, fig.width = 3, fig.height = 2.75}
abs_t_null <- with(
  filter(PlantGrowth, group %in% c("ctrl", "trt2")),
    replicate(10000, 
      abs(t.test(weight ~ sample(group))$statistic)))

ggplot(data_frame(`|t|` = abs_t_null), aes(x = `|t|`)) +  
  geom_histogram(binwidth = 0.1, boundary = 0) +
  geom_vline(xintercept = abs(tt$statistic), col="red")

mean(abs(tt$statistic) <= abs_t_null)
```
%$
```{r tttestpermcheck, echo = FALSE}
stopifnot(abs(mean(abs(tt$statistic) <= abs_t_null) -  tt$p.value) < 4e-4)
```
%
\begin{ques}
Why did we use the absolute value function (\Rfunction{abs}) in the above code?
\end{ques}
\begin{ques}
Plot the (parametric) t-distribution with the appropriate degrees of freedom?
\end{ques}

The $t$-test comes in multiple flavors, all of which can be chosen through parameters of
the \Rfunction{t.test} function. \marginpar{Different flavors of $t$-test} 
What we did above was a two-sided two-sample unpaired test with equal variance.  \emph{Two-sided} refers
to the fact that we were open to reject the null hypothesis if the weight of the treated
plants was either larger or smaller than that of the untreated ones.  \emph{Two-sample}
indicates that we compared the means of two groups to each other; another option would be
to compare the mean of one group against a given, fixed number. \emph{Unpaired} means that
there was no direct 1:1 mapping between the measurements in the two groups. If, on the
other hand, the data had been measured on the same plants before and other treatment,
then a paired test would be more appropriate, as it looks at the change of weight
within each plant, rather than their absolute weights. \emph{Equal variance} refers to the
way the statistic~\eqref{eq:mt:tstat} is calculated. That expression is most appropriate
if the variances within each group are about the same. If they are much different, an
alternative form\footnote{Welch's $t$-test} and associated asymptotic theory exist.

Now let's try something peculiar: duplicate the data.\marginpar{The independence assumption}
```{r ttdup}
with(rbind(PlantGrowth, PlantGrowth),
   t.test(weight[group =="ctrl"], 
          weight[group =="trt2"],
          var.equal = TRUE)) 
```

Note how the estimates of the group means (and thus, of the difference) are unchanged, but
the p-value is now much smaller! We can conclude two things from this:

\begin{itemize}
\item The power of the $t$-test depends on the sample size. Even if the underlying
  biological differences are the same, a dataset with more samples tends to give more
  significant results\footnote{You can already see this from Equation~\ref{eq:mt:cttest}.}.
\item The assumption of independence between the measurements is really important.
  Blatant duplication of the same data is an extreme form of dependence, but to some
  extent the same thing happens if you mix up different levels of replication. For
  instance, suppose you had data from 8 plants, but measured the same thing twice on each
  plant (technical replicates), then pretending that these are now 16 independent
  measurements to a downstream analysis, such as the $t$-test, is wrong.
\end{itemize}

%--------------------------------------------------
\section{P-value Hacking}\label{sec:mt:pvaluehack}
%--------------------------------------------------
```{r prohHead_assertion, echo=FALSE}
stopifnot(probHead!=0.5)
```
%
Let's go back to the coin tossing example. We could not reject the null hypothesis (that
the coin is fair) at a level of 5\% -- even though we ``knew'' that it is unfair. After
all, \Robject{probHead} was `r probHead` on page~\pageref{mt:probHead}. Let's suppose we
now start looking at different test statistics. Perhaps the number of consecutive series
of 3 or more heads. Or the number of heads in the first `r ceiling(numFlips/2)` coin
flips. And so on.  At some point we will find a test that happens to result in a small
p-value, even if just by chance (after all, the probability for the p-value to be less
than 5\% under the null is 0.05, not an infinitesimally small number).  We just did what
is called \eindex{p-value
  hacking}\footnote{\url{http://fivethirtyeight.com/features/science-isnt-broken}}~\cite{Head:PLoSBiol:2015:pvaluehacking}. You
see what the problem is: in our zeal to prove our point we tortured the data until some
statistic did what we wanted. A related tactic is \eindex{hypothesis switching} or
\eindex{HARKing} -- hypothesizing after the results are known: we have a
dataset, maybe we have invested a lot of time and money into assembling it, so we need
results. We come up with lots of different null hypotheses, test them, and iterate, until
we can report something interesting.  \marginpar{\textbf{Avoid fallacy}.  Keep in mind
  that our statistical test is never attempting to prove our null hypothesis is true - we
  are simply saying whether or not there is evidence for it to be false. If a high
  p-value \emph{were} indicative of the truth of the null hypothesis, we could formulate
  a completely crazy null hypothesis, do an utterly irrelevant experiment, collect a small
  amount of inconclusive data, find a p-value that would just be a random number between
  0 and 1 (and so with some high probability above our threshold $\alpha$) and,
  whoosh, our hypothesis would be demonstrated!}

All these tactics are not according to the rule book, as described in
Section~\ref{sec:mt:fivesteps}, with a linear and non-iterative sequence of choosing the
hypothesis and the test, and then seeing the data. But, of course, they are often more
close to reality. With biological data, we tend to have so many different choices for
``normalising'' the data, transforming the data, add corrections for apparent batch
effects, removing outliers, \ldots.  The topic is complex and
open-ended. \citet{Wasserstein2016:ASA} give a very readable short summary of the problems
with how p-values are used in science, and of some of the misconceptions. They also
highlight how p-values can be fruitfully used.  The essential message is: be completely
transparent about your data, what analyses were tried, and how they were done. Provide the
analysis code.  Only with such contextual information can a p-value be useful.
  
%--------------------------------------------------
\section{Multiple Testing}
%--------------------------------------------------
\begin{ques}
Look up \href{http://xkcd.com/882}{xkcd comic 882}. Why didn't the newspaper report the 
results for the other colors?
\end{ques}

The same quandary occurs with high-throughput data in biology. And with force! You will be
dealing not only with 20 colors of jellybeans, but, say, with 20,000 genes that were
tested for differential expression between two conditions, or with 3 billion positions in
the genome where a DNA mutation might have happened. So how do we deal with this?  Let's
look again at our table relating statistical test results with reality
(Table~\ref{tab:mt:typeosferror}), this time framing everything in terms of many null
hypotheses.

\begin{table}
\begin{longtable}[c]{@{}llll@{}}
\hline\noalign{\medskip}
Test vs Reality & \textbf{Null Hypothesis is true} & 
                  \textbf{\ldots is false} & \textbf{Total}\\\noalign{\medskip}
\hline\noalign{\medskip}
\textbf{Rejected}     & V & S & $R$    \\\noalign{\medskip}
\textbf{Not rejected} & U & T & $m - R$\\\noalign{\medskip}
\textbf{Total}        & $m_0$ & $m-m_0$ & $m$ \\\noalign{\medskip}
\hline
\end{longtable}
\caption{Types of error in multiple testing. The letters designate the number of 
times each type of error occurs.\label{tab:mt:mterrors}}
\end{table}

\begin{itemize}
\item $m$: total number of hypotheses
\item $m_0$: number of null hypotheses
\item $V$: number of false positives (a measure of type I error)
\item $T$: number of false negatives (a measure of type II error)
\item $S$, $U$: number of true positives and true negatives
\item $R$: number of rejections
\end{itemize}

%--------------------------------------------------
\section{The Family Wise Error Rate}\label{sec:multtest:FWER}
%--------------------------------------------------
The \eindex{family wise error rate} (FWER) is the probability that $V>0$, i.\,e., that we
make one or more false positive errors. We can compute it as the complement of making no
false positive errors at all\footnote{Assuming independence.}.
\begin{equation}\label{eq:mt:bonferroni}
1 - P(\text{no rejection of any of $m_0$ nulls}) = 1 - (1 - \alpha)^{m_0} \to 1
\quad\text{as } m\to\infty
\end{equation}
For any fixed $\alpha$, this probability is appreciable as soon as $m$ is in the order of $1/\alpha$, and tends towards 1 as $m$ becomes larger. 
This relationship can have big consequences for experiments like DNA matching, where a
large database of potential matches is searched. For example, if there is a one in a million
chance that the DNA profiles of two people match by random error, and your DNA is tested against a
database of 800000 profiles, then the probability of a random hit with the database
(i.\,e., without you being in it) is:
%
```{r typeerror3}
1 - (1 - 1/1e6)^8e5
```
%
That's pretty high. And once the database contains a few million profiles, a false
hit is virtually unavoidable.

\begin{ques}
Prove that the probability \eqref{eq:mt:bonferroni} does indeed become very close to 1 when $m$ is large.
\end{ques}

%--------------------------------------------------
\subsection{Bonferroni correction}
%--------------------------------------------------
How are we to choose the per-hypothesis $\alpha$ if we want FWER control? 
The above computations give us an intuition that the product of $\alpha$ with $m$ gives us a ballpark
estimate, and this guess is in fact true.
The Bonferroni correction is simply that if we want FWER control at level $\alpha_{\text{FWER}}$,
we should choose the per hypothesis threshold $\alpha = \alpha_{\text{FWER}}/m$. Let's check this
out on an example.
%
```{r bonferroni, fig.width = 3, fig.height = 2.75}
m <- 10000

ggplot(data_frame(
  alpha = seq(0, 7e-6, length.out = 100), 
  p     = 1 - (1 - alpha)^m), 
  aes(x = alpha, y = p)) +  geom_line() +
  xlab(expression(alpha)) +
  ylab("Prob( no false rejection )") +
  geom_hline(yintercept = 0.05, col="red")
```
%$
%
In Figure~\ref{fig:mt:bonferroni}, the black line intersects the red line 
(which corresponds to a value of 0.05) at $\alpha=
`r  alpha <- seq(2e-6, 7e-6, length.out=1000); p <- 1-(1-alpha)^m; wh <- which(p<=0.05); stopifnot(length(wh)>=1); alpha[last(wh)] %>% signif(3)`$, 
which is just a little bit more than the value of $0.05/m$ implied by the Bonferroni correction.

\begin{ques}
Why are the two values not exactly the same?
\end{ques}

A potential drawback of this method, however, is that when $m$
is large, the rejection threshold is very small. This means that the individual tests need
to be very powerful if we want to have any chance to detect something. Often this is not
possible, or would not be an effective use of our time and money.  We'll see that there
are more nuanced methods of controlling our type I error.
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_bonferroni-1}
\caption{Bonferroni correction. The plot shows the graph of~\eqref{eq:mt:bonferroni}
for $m=`r m`$ as a function of $\alpha$.\label{fig:mt:bonferroni}}
\end{marginfigure}

%------------------------------------------------------------
\section{The False Discovery Rate}
%------------------------------------------------------------
Let's look at some real data. We load up the RNA-Seq dataset \Robject{airway}, which
contains gene expression measurements (gene-level counts) of four primary human airway
smooth muscle cell lines with and without treatment with dexamethasone, a synthetic
glucocorticoid.  We'll use the \Biocpkg{DESeq2} method that we'll discuss in more detail
in Chapter~\ref{Chap:7}. For now it suffices to say that it performs a test for differential
expression for each gene. Conceptually, the tested null hypothesis is very similar to that of the
$t$-test, although the test statistic and the null distribution are slightly more involved
since we are dealing with count data.
%
```{r mtdeseq2airway, message=FALSE, warning=FALSE}
library("DESeq2")
library("airway")
data("airway")
aw <- DESeqDataSet(se = airway, design = ~ cell + dex) 
aw <- aw[ rowMeans(counts(aw)) > 1, ]
dim(aw)
counts(aw)[1:2, 1:3]
colData(aw)[, 2:4]
awfit <- DESeq(aw)
awde  <- as.data.frame(results(awfit))
```
%
\begin{ques}
  Why did we (in the $5^{\text{th}}$ line of the above code chunk) remove genes that have a
  very small number of counts on average across all samples?
\end{ques}
\begin{ques}
  Have a look at the content of \Robject{awde}.
\end{ques}
\begin{ques}
  (Optional) Consult the \Biocpkg{DESeq2} vignette and/or Chapter~\ref{Chap:7} for more information
  on what the above code chunk does.
\end{ques}

%--------------------------------------------------
\subsection{The p-value histogram}
%--------------------------------------------------
Let's plot the histogram of p-values.
%
```{r awpvhist, fig.width = 3, fig.height = 2.75}
ggplot(awde, aes(x = pvalue)) + 
  geom_histogram(binwidth = 0.025, boundary = 0)
```
%$
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_awpvhist-1}
\caption{p-value histogram of for the \Robject{airway} data.\label{fig:mt:awpvhist}}
\end{marginfigure}
%
The histogram (Figure~\ref{fig:mt:awpvhist}) is an important sanity check for any analysis
that involves multiple tests. We expected it to be composed of two components:
\begin{itemize}
\item A uniform background, which corresponds to the null hypotheses. Remember that under
  the null, the p-value is distributed uniformly in $[0,1]$.
\item A peak at the left, from small p-values that were emitted by the alternatives.
\end{itemize}
The relative size of these two components depends on the fraction of true nulls and true
alternatives in the data. The shape of the peak towards the left depends on the power of
the tests: if the experiment was underpowered, we can still expect that the
p-values from the alternatives tend towards being small, but some of them will scatter up into
the middle of the range.

\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_awpvvisfdr-1}
\caption{Visual estimation of the FDR with the p-value histogram.\label{fig:mt:awpvvisfdr}}
\end{marginfigure}
%
Suppose we reject all tests with a p-value less than $\alpha$. We could visually determine
an estimate of null hypotheses among these with a plot like in Figure~\ref{fig:mt:awpvvisfdr}
%
```{r awpvvisfdr, fig.width = 3, fig.height = 2.75}
alpha <- binw <- 0.025
pi0 <- 2 * mean(awde$pvalue > 0.5)
ggplot(awde, 
  aes(x = pvalue)) + geom_histogram(binwidth = binw, boundary = 0) +
  geom_hline(yintercept = pi0 * binw * nrow(awde), col = "blue") +
  geom_vline(xintercept = alpha, col = "red")
```
%$
%
We see that there are `r sum(awde$pvalue<=alpha)` %$
p-values in the first bin ($[0,\alpha]$), among which we expect around
`r round(pi0 * alpha * nrow(awde))` to be nulls (as indicated by the blue line). 
Thus we can estimate the fraction of false rejections as
```{r fdrvis}
pi0 * alpha / mean(awde$pvalue <= alpha) 
```
%$

Coming back to our terminology of Table~\ref{tab:mt:mterrors}, the \eindex{false discovery
  rate} (FDR) is defined as
%
\begin{equation}\label{eq:mt:deffdr}
\text{FDR} = \text{E}\!\left [\frac{V}{\max(R, 1)}\right ],
\end{equation}
%
The expression in the denominator makes sure that the maths are well-defined even when
$R=0$\footnote{\ldots and thus by implication $V=0$.}.  
$\text{E[\;]}$ stands for the \eindex{expectation value}. That means that the FDR is
not a quantity associated with a specific outcome of $V$ and $R$ for one particular
experiment.  Rather, given our choice of tests and associated rejection rules for them, it
is the average\footnote{Since the FDR is an expectation value, it does not provide worst
  case control: in any single experiment, the so-called false discovery proportion (FDP),
  that is $V/R$ without the $\text{E[\;]}$, could be much higher (or lower). Just
  as knowing the mean of a population does not tell you the values of the extremes.}
proportion of type I errors out of the rejections made, where the average is taken (at
least conceptually) over many replicate instances of the experiment.

%-------------------------------------------------------------------------
\subsection{The Benjamini-Hochberg algorithm for controlling the FDR}
%-------------------------------------------------------------------------

There is a more elegant alternative to the ``visual FDR'' method of the 
last section. The procedure, introduced by Y.~Benjamini and Y.~Hochberg\cite{BH:1995}
has these steps:
%
\begin{itemize}
\item
  First, order the p-values in increasing order, $p_1 \ldots p_m$
\item Then for some choice of $\varphi$ (our target FDR), find the largest value of $k$
  that satisfies: $p_{k} \leq \varphi \, k / m$
\item
  Finally reject the hypotheses $1 \ldots k$
\end{itemize}

We can see how this procedure works when applied to our RNA-seq p-values
through a simple graphical illustration:

```{r benjaminihochberg, fig.width = 3, fig.height = 2.75}
phi  <- 0.10 
awde <- mutate(awde, rank = rank(pvalue)) 
m    <- nrow(awde)

ggplot(filter(awde, rank <= 7000), aes(x = rank, y = pvalue)) + 
  geom_line() + geom_abline(slope = phi / m, col="red")
```
%
\begin{marginfigure}[-20mm]
\includegraphics[width=\linewidth]{chap14-r_benjaminihochberg-1}
\caption{Visualisation of the Benjamini-Hochberg procedure.
Shown is a zoom-in to the 7000 lowest p-values.\label{fig:mt:BH}}
\end{marginfigure}
%
The method now simply finds the rightmost point where the black (our p-values) 
and red lines (slope $\varphi / m$) intersect. 
Then it rejects all tests to the left.
%
```{r kmax} 
kmax <- with(arrange(awde, rank), 
         last(which(pvalue <= phi * rank / m)))
kmax
```
%
\begin{ques}
  Compare the value of \Robject{kmax} with the number of `r sum(alpha>=awde$pvalue)` %$
from above (Figure~\ref{fig:mt:awpvvisfdr}). Why are they different?
\end{ques}
\begin{ques}
  Look at the code associated with the option \Robject{method="BH"} of the
  \Rfunction{p.adjust} function that comes with R. Compare it to what we did above.
\end{ques}

%------------------------------------------------------------
\section{The Local FDR}
%------------------------------------------------------------
\begin{figure}
\begin{center}
\includegraphics[width=0.62\textwidth,height=\textheight,keepaspectratio]{xkcd1132.png}
\end{center}
\caption{From \url{http://xkcd.com/1132} -- While the frequentist only has the currently available
data, the Bayesian can draw on mechanistic insight or on previous experience. As a Bayesian,
she would know enough about physics to understand that our sun's mass is too small to 
become a nova. And if she does not know physics, she might be an \emph{empirical Bayesian},
and draw her prior from countless previous days where the sun did not go 
nova. \label{fig:mt:sunexplode}}
\end{figure}
%
While the xkcd comic mentioned in Figure~\ref{fig:mt:jellybeans} ends with a rather
sinister intepretation of the multiple testing problem as a way to accumulate errors,
Figure~\ref{fig:mt:sunexplode} highlights the multiple testing opportunity: when we do
many tests, we can use the data to increase our understanding beyond what's possible with
a single test.
%
```{r lfdr, fig.width = 3, fig.height = 6, echo = FALSE}
pi0 <- 0.6
f1 <- function(t, shape2 = 7) {
   rv <- dbeta(t, 1, shape2)
   rv / sum(rv) * (length(rv)-1) * (1-pi0) 
}

t <- seq(0, 1, length.out = 101)
t0 <- 0.1

f0  <- rep(pi0, length(t))
f   <- f0 + f1(t)
F0  <- cumsum(f0) / (length(t)-1)
F   <- cumsum(f)  / (length(t)-1)
stopifnot(abs(F[length(F)] - 1) < 1e-2)

myplot <- function(y, y0, ylim, yat, havepi0, colo = brewer.pal(12, "Paired")) {
  plot(x = t, y = y, type = "l", xlim = c(0, 1), ylim = ylim,
    xaxs = "i", yaxs = "i", ylab = "", yaxt = "n", xaxt = "n", xlab = "", main = deparse(substitute(y)))
  axis(side = 1, at = c(0, 1))
  axis(side = 2, at = yat)
  xa  <-  t[t<=t0]
  xb  <-  t[t>=t0]
  y0a <- y0[t<=t0]
  y0b <- y0[t>=t0]
  ya  <-  y[t<=t0]
  polygon(x = c(xa, rev(xa)), y = c(y[t<=t0], rev(y0a)), col = colo[2])
  polygon(x = c(xb, rev(xb)), y = c(y[t>=t0], rev(y0b)), col = colo[1])
  polygon(x = c(xa, rev(xa)), y = c(rep(0, length(xa)), rev(y0a)), col = "#c0c0c0")
  polygon(x = c(xb, rev(xb)), y = c(rep(0, length(xb)), rev(y0b)), col = "#f0f0f0")
  segments(x0 = rep(t0, 2), x1 = rep(t0, 2), y0 = c(0, last(y0a)), y1 = c(last(y0a), last(ya)),
           col = colo[5:6], lwd = 3)
  text(t0, 0, adj = c(0, 1.8), labels = expression(p), cex = 1, xpd = NA)
  if (havepi0)
      text(0, pi0, adj = c(1.5, 0.5), labels = expression(pi[0]), cex = 1, xpd = NA)
}

par(mai = c(1, 0.6, 0.4, 0.3), mfcol = c(2,1))
myplot(f, f0, ylim = c(0, f[1]), yat = c(0:3),       havepi0 = TRUE)
myplot(F, F0, ylim = c(0, 1),    yat = c(0, 0.5, 1), havepi0 = FALSE)
```
%
Let's get back to the histogram in Figure~\ref{fig:mt:awpvvisfdr}. Conceptually, we can
think of it in terms of the two-groups model~\cite{Efron2010}:
%
\begin{equation}\label{eq:mt:twogroups}
f(p)= \pi_0  + (1-\pi_0) f_{\text{alt}}(p),
\end{equation}
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_lfdr-1}
\caption{Local false discovery rate and the two-group model, with some 
choice of $f_{\text{alt}}(p)$, and $\pi_0=`r pi0`$.
Top: densities, bottom: distribution functions.\label{fig:mt:lfdr}}
\end{marginfigure}
%
Here, $f(p)$ is the density of the distribution (what the histogram would look like with
infinitely much data and infinitely small bins), $\pi_0$ is a number between 0 and 1 that
represents the size of the uniform component, and $f_{\text{alt}}$ is the alternative
component. These functions are visualised in the upper panel of Figure~\ref{fig:mt:lfdr}:
the blue areas together correspond to the graph of $f_{\text{alt}}(p)$, the grey areas to
that of $f_{\text{null}}(p) = \pi_0$. If we now consider one particular cutoff $p$ (say,
$p=`r t0`$ as in Figure~\ref{fig:mt:lfdr}), then we can decompose the value of $f$ at
the cutoff (red line) into the contribution from the nulls (light red, $\pi_0$) and from the alternatives 
(darker red, $(1-\pi_0) f_{\text{alt}}(p)$). So we have the \eindex{local false disovery rate}
\begin{equation}
\text{fdr}(p) = \frac{\pi_0}{f(p)},
\end{equation}
and this quantity, which by definition is between 0 and 1, tells us the probability that a
hypothesis which we rejected at some cutoff $p$ would be a false positive. Note how the
$\text{fdr}$ in Figure~\ref{fig:mt:lfdr} is a monotonically decreasing function of $p$,
and this goes with our intuition that the fdr should be lowest for the smallest $p$ and then
gradually get larger, until it reaches 1 at the very right end.  We can make a similar
decomposition not only for the red line, but also for the area under the curve. This is
\begin{equation}
F(p)  = \int_0^p f(t)\,dt,
\end{equation}
and the ratio of the dark grey area (that is, $\pi_0$ times $p$) to that is the
\eindex{tail area false disovery rate} (Fdr\footnote{The convention is to use the lower
  case abbreviation fdr for the local, and the abbreviation Fdr for the tail-area false
  discovery rate in the context of the two-groups model~\eqref{eq:mt:twogroups}. The
  abbreviation FDR is used for the original definition~\eqref{eq:mt:deffdr}, which is a
  bit more general.}).
%
\begin{equation}
\text{Fdr}(p) = \frac{\pi_0\,p}{F(p)},
\end{equation}
%
We'll use the data version of $F$ for diagnostics in Figure~\ref{fig:mt:awde:stratified:ecdf}.

The packages \Biocpkg{qvalue} and \CRANpkg{fdrtool} offer facilities to fit these models to data.
%
```{r fdrtool, fig.width = 4, fig.height = 8, message = FALSE, results = "hide"}
library("fdrtool")
ft <- fdrtool(awde$pvalue, statistic = "pvalue")
```
%$
In \CRANpkg{fdrtool}, what we called $\pi_0$ above is called \Robject{eta0}:
%
```{r qvalue31}
ft$param[,"eta0"]
```
%$
\begin{ques}
What do the plots show that are produced by the above call to \Rfunction{fdrtool}?
\end{ques}
\begin{ques}
Explore the other elements of the \Rclass{`r class(ft)`} \Robject{ft}.
\end{ques}
\begin{ques}
What does the \emph{empirical} in empirical Bayes methods stand for?
\end{ques}

% \begin{marginfigure}[+30mm]
% \includegraphics[width=\linewidth]{chap14-r_fdrtool-1}
% \caption{Output of \Rfunction{fdrtool}. The graphs shown here correspond to the various
%   components of the two-groups model. Note that the $x$-axis is inverted in comparison to
%   Figure~\ref{fig:mt:lfdr}, with smallest p-values to the right.\label{fig:mt:fdrtool}}
% \end{marginfigure}

%--------------------------------------------------
\subsection{Local versus total}
%--------------------------------------------------
%
The FDR (or the Fdr) is a set property - it is a single number that applies to a whole set
of rejections made in the course of a multiple testing analysis. In contrast, the fdr is a
local property - it applies to individual additional hypothesis. Recall Figure~\ref{fig:mt:lfdr}, where the fdr
was computed for each point along the $x$-axis of the density plot, whereas the Fdr
depends on the areas to the left of the red line.

\begin{ques}
Check out the concepts of \emph{total cost} and \emph{marginal cost} in economics.
Can you seen an analogy with Fdr and fdr?
\end{ques}

%--------------------------------------------------
\section{Independent Filtering and Hypothesis Weighting}
%--------------------------------------------------
The Benjamini-Hochberg method and the two-groups model, as we have seen them so far,
implicitly assume \emph{exchangeability} of the hypotheses: all we use are the p-values.
Beyond these, we do not take into account any additional information. This is not always optimal.

Let's look at an example. Intuitively, the signal-to-noise ratio for genes with larger
numbers of reads mapped to them should be better than for genes with few reads, and that
should affect the power of our tests. We look at the mean of normalized counts across
samples. In the \Biocpkg{DESeq2} software this quantity is called the \Robject{baseMean}.
%
\begin{marginfigure}[-20mm]
\includegraphics[width=\linewidth]{chap14-r_awde_basemean_hist-1}
\caption{Histogram of \Robject{baseMean}. We see that it covers a 
large dynamic range, from close to 0 to around
`r round(max(awde$baseMean), -4)`.%$
\label{fig:mt:basemean_hist}}
\end{marginfigure}
%
```{r awde_basemean_counts}
awde$baseMean[1]
cts <- counts(awfit, normalized = TRUE)[1, ]
cts
mean(cts)
```
```{makesure, echo=FALSE}
stopifnot(abs(mean(cts)-awde$baseMean[1])<1e-9)
```
%$
\begin{marginfigure}[+0mm]
\includegraphics[width=\linewidth]{chap14-r_awde_basemean_scp-1}
\caption{Scatterplot of the rank of \Robject{baseMean}
versus the negative logarithm of the p-value. For small values of \Robject{baseMean},
no small p-values occur. Only for genes whose read counts across all samples 
have a certain size, the test for differential expression has power to come 
out with a small p-value.
\label{fig:mt:basemean_scp}}
\end{marginfigure}
%
Next we produce its histogram across genes, and a scatterplot between it and the p-values.
%
```{r awde_basemean_hist, fig.width = 3, fig.height = 2}
ggplot(awde, aes(x = asinh(baseMean))) + 
  geom_histogram(bins = 60)
```
%$
```{r awde_basemean_scp, fig.width = 3, fig.height = 2.6}
ggplot(awde, aes(x = rank(baseMean), y = -log10(pvalue))) +
  geom_hex(bins = 60) +
  theme(legend.position = "none")
```
%
\begin{ques}
Why did we use the $\text{asinh}$ transformation for the histogram? How does it look like with no
transformation, the logarithm, the shifted logarithm, i.\,e., $\log(x+\text{const.})$?
\end{ques}
\begin{ques}
In the scatterplot, why did we use $-\log_{10}$ for the p-values? Why the rank transformation 
for the \Robject{baseMean}?
\end{ques}
%
For convenience, we discretize \Robject{baseMean} into a factor variable \Robject{group},
which corresponds to six equal-sized groups.
%
```{r awde_stratify}
awde <- mutate(awde, stratum = cut(baseMean, 
   breaks = quantile(baseMean, probs = 
              seq(0, 1, length.out = 7)),
   include.lowest = TRUE))
```
%
In Figures~\ref{fig:mt:awde:stratified:hist} and \ref{fig:mt:awde:stratified:ecdf} we 
see the histograms of p-values and the ECDFs stratified by \Robject{stratum}.
%
```{r awde_stratified_hist, fig.width = 3.5, fig.height = 5}
ggplot(awde, aes(x = pvalue)) + 
  geom_histogram(binwidth = 0.025, boundary = 0) +
  facet_wrap( ~ stratum, nrow = 4)
```
```{r awde_stratified_ecdf, fig.width = 5, fig.height = 2.9}
ggplot(awde, aes(x = pvalue, col = stratum)) + 
  stat_ecdf(geom = "step")
```
%$
%
\begin{marginfigure}[-50mm]
\includegraphics[width=\linewidth]{chap14-r_awde_stratified_hist-1}
\caption{p-value histograms of the airway data, stratified 
into `r length(unique(awde$stratum))`%$ 
equally sized groups defined by increasing value of \Robject{baseMean}.\label{fig:mt:awde:stratified:hist}}
\end{marginfigure}
%
If we were to fit the two-group model to these strata separately, we would get quite
different parameters (i.\,e., $\pi_0, f_{\text{alt}}$). 
For the most lowly expressed genes (those in the first
\Robject{baseMean}-bin), the power of the \Biocpkg{DESeq2}-test is low, and the
p-values
essentially all come from the null component. As we go higher in average expression, the
height of the small-p-values peak in the histograms increases, reflecting the increasing 
power of the test. 

Can we use that for a better multiple testing correction?  It turns
out that this is possible. We can use either \eindex{independent filtering}~\cite{Bourgon:2010:PNAS}
or \eindex{independent hypothesis weighting} (IHW)~\cite{Ignatiadis:2016}.
%
\begin{marginfigure}[-20mm]
\includegraphics[width=\linewidth]{chap14-r_awde_stratified_ecdf-1}
\caption{Same data as in Figure~\ref{fig:mt:awde:stratified:hist}, 
shown with ECDFs.\label{fig:mt:awde:stratified:ecdf}}
\end{marginfigure}
%

```{r ihw_do}
library("IHW")
ihw_res <- ihw(awde$pvalue, awde$baseMean, alpha = 0.1)
rejections(ihw_res)
```
Let's compare this to what we get from the ordinary (unweighted) Benjamini-Hochberg method:
```{r ihwcompare}
padj_BH <- p.adjust(awde$pvalue, method = "BH")
sum(padj_BH < 0.1)
```
%$
With hypothesis weighting, we get more rejections. For these data, the difference is
notable though not spectacular, this is because their signal-to-noise is already quite
high. In other situations (e.\,g., when there are fewer replicates or they are more
noisy, or when the effect of the treatment is less drastic), the difference from using IHW can
be more pronounced.

We can have a look at the weights determined by the \Rfunction{ihw} function.
%
```{r ihw_plot, fig.width = 4.2, fig.height = 2.3}
plot(ihw_res)
```
%$
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap14-r_ihw_plot-1}
\caption{Hypothesis weights determined by the \Rfunction{ihw} function.  Here the
  function's default settings chose `r nrow(weights(ihw_res, levels_only=TRUE))` strata, while
  in our manual exploration above (Figures~\ref{fig:mt:awde:stratified:hist},
  \ref{fig:mt:awde:stratified:ecdf}) we had used 
  `r nlevels(awde$stratum)`; in practice, this is a minor detail.\label{fig:mt:ihwplot}}
\end{marginfigure}
%$
%
Intuitively, what happens here is that IHW chooses to put more weight on the hypothesis
strata with higher \Robject{baseMean}, and low weight on those with very low counts. The
Benjamini-Hochberg method has a certain type-I error budget, and rather than spreading
it equally among all hypotheses, here we take it away from those strata that have little change
of small fdr anyway, and "invest" it in strata where many hypotheses can be rejected at
small fdr.

\begin{ques}
Why does Figure~\ref{fig:mt:ihwplot} show `r ncol(weights(ihw_res, levels_only=TRUE))` curves,
rather than only one?
\end{ques}

Such possibilities for stratification by a covariate (in our case: \Robject{baseMean})
exist in many multiple testing situations. Informally, we need the covariate to be 
\begin{itemize}
\item statistically independent from our p-values under the null, but 
\item informative of the prior probability $\pi_0$ and/or the power of the test 
(the shape of the alternative density,  $f_{\text{alt}}$) in the two-groups model.
\end{itemize}
These requirements can be assessed through diagnostic plots as in
Figures~\ref{fig:mt:basemean_hist}--\ref{fig:mt:awde:stratified:ecdf}.

%--------------------------------------------------
\section{Summary of this Chapter}
%--------------------------------------------------

To summarize what we hope you've learned from this chapter:

\begin{itemize}
\item Understand the principal steps of a hypothesis test.
\item Know the different types of errors we are about to commit when doing
  hypothesis testing.
\item Understand the challenges and opportunities of doing thousands or millions of tests.
\item Know your different between the family wise error rate and the false discovery rate.
\item Be familiar with the false discovery rate, and understand the difference between its
  local and total (tail-area) definitions.
\item Understand that often not all hypotheses are exchangeable, and that taking into
  account informative covariates can improve your analyses.
\item Be familiar with diagnostic plots, and know to always look at the p-value histogram
  when encountering a multiple testing analysis.
\end{itemize}

%--------------------------------------------------
\section{Exercises}
%--------------------------------------------------
\begin{ex}
  What is a data type or an analysis method from your scientific field of expertise that
  relies on multiple testing? Do you focus on FWER or FDR? Are the hypotheses all
  exchangeable, or are there any informative covariates?
\end{ex}
\begin{ex}
  Why do statisticians often focus so much on the null hypothesis of a test, compared to
  the alternative hypothesis?
\end{ex}
\begin{ex}
  How can we ever prove that the null hypothesis is true? Or that the alternative is true?
\end{ex}
\begin{ex}
  Make a less extreme example of correlated test statistics than the data duplication at
  the end of Section~\ref{sec:mt:ttest}. Simulate data with true null hypotheses only, so
  that the data morph from being completely independent to totally correlated as a
  function of some continuous-valued control parameter. Check type-I error control
  (e.\,g., with the p-value histogram) as a function of this control parameter.
\end{ex}
\begin{ex}
  Find an example in the published literature that looks like p-value hacking, outcome
  switching, HARKing played a role.
\end{ex}
\begin{ex}
  What other type-I and type-II error concepts are there for multiple testing?
\end{ex}
\begin{ex}
  The FDR is an expectation value, i.\,e., aims to control average behavior of a procedure. 
  Are there methods for worst case control?
\end{ex}

%--------------------------------------------------
\section{Further Reading}
%--------------------------------------------------
\begin{itemize}
\item A comprehensive text book treatment of multiple testing is given by \cite{Efron2010}.
\item Outcome switching in clinical trials: \url{http://compare-trials.org}
\item For hypothesis weighting, the \Biocpkg{IHW} vignette, the IHW
  paper~\cite{Ignatiadis:2016} and the references therein.
\end{itemize}


% \section{Session Info}
% ```{r sessionInfo}
% sessionInfo()
% ```
